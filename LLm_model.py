# from llama_cpp import Llama
# llm_model = None

def init_model():
    return
    # global llm_model   
    # # 4) Load your LLM once
    # llm_model = Llama(
    #     model_path="models\models--Aspen77--llama-3-3b-instruct-extract-pii-Q4_K_M-GGUF\snapshots\cdff9f7a3a04452147c6d6e6a9f76f6d783c376f\llama-3-3b-instruct-extract-pii-q4_k_m.gguf",
    #     n_threads=4,
    #     verbose=True,
    # )